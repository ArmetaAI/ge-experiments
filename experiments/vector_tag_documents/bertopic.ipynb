{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Document Vectorization & Tag Evaluation\n",
    "\n",
    "This notebook experiments with BERTopic for creating vector representations of documents from GCS bucket.\n",
    "It evaluates whether assigned tags are correctly predicted based on document content.\n",
    "\n",
    "## Setup:\n",
    "- GCS Bucket: `gosexpert_categorize`\n",
    "- Database: PostgreSQL with pgvector extension\n",
    "- Split: 70% train / 30% test\n",
    "- Text extraction: First couple pages of each document\n",
    "- MLflow: Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Data manipulation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# PDF processing\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "\n",
    "# BERTopic and embeddings\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Database\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# GCS and MLflow\n",
    "from gcs_bucket_interface import GCSBucketInterface\n",
    "from mlflow_recorder import MLflowRecorder\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENT_NAME = \"bertopic_tag_eval_v1\"\n",
    "BUCKET_NAME = \"gosexpert_categorize\"\n",
    "TRAIN_TEST_SPLIT = 0.3  # 30% test, 70% train\n",
    "RANDOM_STATE = 42\n",
    "PAGES_TO_EXTRACT = 3  # Extract first 3 pages from each document\n",
    "\n",
    "# BERTopic configuration\n",
    "EMBEDDING_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "MIN_TOPIC_SIZE = 10\n",
    "N_NEIGHBORS = 15\n",
    "N_COMPONENTS = 5\n",
    "\n",
    "# Database configuration from .env\n",
    "DB_CONFIG = {\n",
    "    'host': '127.0.0.1',\n",
    "    'port': 5433,\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASS')\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Database: {DB_CONFIG['database']}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Train/Test Split: {int((1-TRAIN_TEST_SPLIT)*100)}/{int(TRAIN_TEST_SPLIT*100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Documents from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCS interface\n",
    "gcs_interface = GCSBucketInterface(bucket_name=BUCKET_NAME)\n",
    "\n",
    "# List all files with metadata\n",
    "print(\"Loading files from GCS bucket...\")\n",
    "all_files = gcs_interface.list()\n",
    "\n",
    "print(f\"Total files found: {len(all_files)}\")\n",
    "\n",
    "# Filter for PDF files and files with metadata tags\n",
    "pdf_files_with_tags = []\n",
    "for file_info in all_files:\n",
    "    if file_info['name'].lower().endswith('.pdf') and file_info.get('metadata'):\n",
    "        # Check if metadata contains tag information\n",
    "        metadata = file_info['metadata']\n",
    "        if any(key.startswith('tag') or 'category' in key.lower() for key in metadata.keys()):\n",
    "            pdf_files_with_tags.append(file_info)\n",
    "\n",
    "print(f\"PDF files with tags: {len(pdf_files_with_tags)}\")\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df_files = pd.DataFrame([{\n",
    "    'file_name': f['name'],\n",
    "    'size': f['size'],\n",
    "    'gcs_uri': f['gcs_uri'],\n",
    "    'metadata': f['metadata'],\n",
    "    'tags': [v for k, v in f['metadata'].items() if 'tag' in k.lower() or 'category' in k.lower()]\n",
    "} for f in pdf_files_with_tags])\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_files.shape}\")\n",
    "print(f\"\\nSample files:\")\n",
    "display(df_files.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Text from PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_blob(blob_name: str, max_pages: int = PAGES_TO_EXTRACT) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from the first N pages of a PDF file in GCS.\n",
    "    \n",
    "    Args:\n",
    "        blob_name: Name of the blob in GCS bucket\n",
    "        max_pages: Maximum number of pages to extract\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get blob from bucket\n",
    "        blob = gcs_interface.bucket.blob(blob_name)\n",
    "        \n",
    "        # Download as bytes\n",
    "        pdf_bytes = blob.download_as_bytes()\n",
    "        \n",
    "        # Read PDF\n",
    "        pdf_file = BytesIO(pdf_bytes)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        # Extract text from first N pages\n",
    "        text = \"\"\n",
    "        num_pages = min(len(pdf_reader.pages), max_pages)\n",
    "        \n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {blob_name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Extract text from all documents\n",
    "print(\"Extracting text from PDF documents...\")\n",
    "texts = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, row in df_files.iterrows():\n",
    "    text = extract_text_from_pdf_blob(row['file_name'])\n",
    "    if text:  # Only keep documents with successfully extracted text\n",
    "        texts.append(text)\n",
    "        valid_indices.append(idx)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df_files)} documents...\")\n",
    "\n",
    "# Filter DataFrame to only include documents with extracted text\n",
    "df_files = df_files.loc[valid_indices].reset_index(drop=True)\n",
    "df_files['text'] = texts\n",
    "df_files['text_length'] = df_files['text'].str.len()\n",
    "\n",
    "print(f\"\\nSuccessfully extracted text from {len(df_files)} documents\")\n",
    "print(f\"Average text length: {df_files['text_length'].mean():.0f} characters\")\n",
    "print(f\"Median text length: {df_files['text_length'].median():.0f} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample text (first 500 chars):\")\n",
    "print(df_files['text'].iloc[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels (extract primary tag from metadata)\n",
    "def get_primary_tag(tags_list):\n",
    "    \"\"\"Get the first/primary tag from tags list.\"\"\"\n",
    "    if isinstance(tags_list, list) and len(tags_list) > 0:\n",
    "        return tags_list[0]\n",
    "    return \"unknown\"\n",
    "\n",
    "df_files['primary_tag'] = df_files['tags'].apply(get_primary_tag)\n",
    "\n",
    "# Remove documents without valid tags\n",
    "df_files = df_files[df_files['primary_tag'] != \"unknown\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Documents with valid tags: {len(df_files)}\")\n",
    "print(f\"\\nTag distribution:\")\n",
    "print(df_files['primary_tag'].value_counts())\n",
    "\n",
    "# Check which tags have enough samples for stratified split (need at least 2 per class)\n",
    "tag_counts = df_files['primary_tag'].value_counts()\n",
    "rare_tags = tag_counts[tag_counts < 2].index\n",
    "valid_tags = tag_counts[tag_counts >= 2].index\n",
    "\n",
    "if len(rare_tags) > 0:\n",
    "    print(f\"\\n⚠️ Warning: {len(rare_tags)} tags have < 2 samples and will be added to train set only:\")\n",
    "    for tag in rare_tags:\n",
    "        print(f\"  - {tag}: {tag_counts[tag]} sample(s)\")\n",
    "    \n",
    "    # Separate rare and valid tag documents\n",
    "    rare_tags_df = df_files[df_files['primary_tag'].isin(rare_tags)].copy()\n",
    "    df_files_for_split = df_files[df_files['primary_tag'].isin(valid_tags)].copy()\n",
    "    \n",
    "    print(f\"\\nProceeding with stratified split on {len(df_files_for_split)} documents...\")\n",
    "    \n",
    "    # Perform stratified split only on valid tags\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_files_for_split, \n",
    "        test_size=TRAIN_TEST_SPLIT, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=df_files_for_split['primary_tag']\n",
    "    )\n",
    "    \n",
    "    # Add rare tag documents to training set\n",
    "    print(f\"Adding {len(rare_tags_df)} rare-tag documents to training set...\")\n",
    "    train_df = pd.concat([train_df, rare_tags_df], ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nAll tags have sufficient samples for stratified split\")\n",
    "    \n",
    "    # Perform standard stratified split\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_files, \n",
    "        test_size=TRAIN_TEST_SPLIT, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=df_files['primary_tag']\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal split:\")\n",
    "print(f\"Train set size: {len(train_df)} ({len(train_df)/(len(train_df) + len(test_df))*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(test_df)} ({len(test_df)/(len(train_df) + len(test_df))*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain tag distribution:\")\n",
    "print(train_df['primary_tag'].value_counts())\n",
    "\n",
    "print(f\"\\nTest tag distribution:\")\n",
    "print(test_df['primary_tag'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize BERTopic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Initialize BERTopic with custom configuration\n",
    "print(\"Initializing BERTopic model...\")\n",
    "\n",
    "# Vectorizer for better Russian/multilingual support\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=None,  # Can add custom stop words if needed\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=MIN_TOPIC_SIZE,\n",
    "    n_gram_range=(1, 2),\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"BERTopic model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'size', 'gcs_uri', 'metadata', 'tags', 'primary_tag'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Embeddings and Train BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for training set...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.coding\\work\\ARMETA\\new_features\\get_title_from_document\\ge-experiments\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate embeddings for train set\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating embeddings for training set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_texts = \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.tolist()\n\u001b[32m      4\u001b[39m train_embeddings = embedding_model.encode(\n\u001b[32m      5\u001b[39m     train_texts, \n\u001b[32m      6\u001b[39m     show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     batch_size=\u001b[32m32\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain embeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_embeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.coding\\work\\ARMETA\\new_features\\get_title_from_document\\ge-experiments\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.coding\\work\\ARMETA\\new_features\\get_title_from_document\\ge-experiments\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for train set\n",
    "print(\"Generating embeddings for training set...\")\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_embeddings = embedding_model.encode(\n",
    "    train_texts, \n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "\n",
    "# Fit BERTopic on training data\n",
    "print(\"\\nFitting BERTopic model on training data...\")\n",
    "topics, probs = topic_model.fit_transform(train_texts, train_embeddings)\n",
    "\n",
    "print(f\"\\nNumber of topics discovered: {len(set(topics)) - 1}\")  # -1 for outlier topic\n",
    "print(f\"Number of outliers: {sum(1 for t in topics if t == -1)}\")\n",
    "\n",
    "# Add topic assignments to train DataFrame\n",
    "train_df = train_df.copy()\n",
    "train_df['bertopic_topic'] = topics\n",
    "train_df['bertopic_prob'] = [p.max() if len(p) > 0 else 0.0 for p in probs]\n",
    "\n",
    "# Display topic info\n",
    "print(\"\\nTop topics:\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "display(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Embeddings for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for test set\n",
    "print(\"Generating embeddings for test set...\")\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_embeddings = embedding_model.encode(\n",
    "    test_texts, \n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "\n",
    "# Transform test data using fitted model\n",
    "print(\"\\nTransforming test data with fitted BERTopic model...\")\n",
    "test_topics, test_probs = topic_model.transform(test_texts, test_embeddings)\n",
    "\n",
    "# Add topic assignments to test DataFrame\n",
    "test_df = test_df.copy()\n",
    "test_df['bertopic_topic'] = test_topics\n",
    "test_df['bertopic_prob'] = [p.max() if len(p) > 0 else 0.0 for p in test_probs]\n",
    "\n",
    "print(f\"Test set topic distribution:\")\n",
    "print(test_df['bertopic_topic'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup PostgreSQL with pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL\n",
    "print(\"Connecting to PostgreSQL database...\")\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "register_vector(conn)\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Connected successfully!\")\n",
    "\n",
    "# Create table for document vectors\n",
    "table_name = \"document_vectors_experiment\"\n",
    "\n",
    "print(f\"\\nCreating table '{table_name}'...\")\n",
    "\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        file_name TEXT NOT NULL,\n",
    "        gcs_uri TEXT NOT NULL,\n",
    "        assigned_tag TEXT NOT NULL,\n",
    "        bertopic_topic INTEGER,\n",
    "        bertopic_prob FLOAT,\n",
    "        embedding vector({train_embeddings.shape[1]}),\n",
    "        text_excerpt TEXT,\n",
    "        dataset_split TEXT,  -- 'train' or 'test'\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create index for vector similarity search\n",
    "print(\"Creating vector index...\")\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS {table_name}_embedding_idx \n",
    "    ON {table_name} \n",
    "    USING ivfflat (embedding vector_cosine_ops)\n",
    "    WITH (lists = 100)\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Store Vectors in pgvector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_vectors_batch(df, embeddings, dataset_split: str):\n",
    "    \"\"\"\n",
    "    Insert document vectors into database.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with document information\n",
    "        embeddings: Document embeddings array\n",
    "        dataset_split: 'train' or 'test'\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for idx, row in df.iterrows():\n",
    "        embedding_idx = df.index.get_loc(idx)\n",
    "        values.append((\n",
    "            row['file_name'],\n",
    "            row['gcs_uri'],\n",
    "            row['primary_tag'],\n",
    "            int(row['bertopic_topic']),\n",
    "            float(row['bertopic_prob']),\n",
    "            embeddings[embedding_idx].tolist(),\n",
    "            row['text'][:1000],  # Store first 1000 chars as excerpt\n",
    "            dataset_split\n",
    "        ))\n",
    "    \n",
    "    execute_values(\n",
    "        cur,\n",
    "        f\"\"\"\n",
    "        INSERT INTO {table_name} \n",
    "        (file_name, gcs_uri, assigned_tag, bertopic_topic, bertopic_prob, \n",
    "         embedding, text_excerpt, dataset_split)\n",
    "        VALUES %s\n",
    "        \"\"\",\n",
    "        values\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "# Insert training vectors\n",
    "print(f\"Inserting {len(train_df)} training vectors...\")\n",
    "insert_vectors_batch(train_df, train_embeddings, 'train')\n",
    "print(\"Training vectors inserted!\")\n",
    "\n",
    "# Insert test vectors\n",
    "print(f\"\\nInserting {len(test_df)} test vectors...\")\n",
    "insert_vectors_batch(test_df, test_embeddings, 'test')\n",
    "print(\"Test vectors inserted!\")\n",
    "\n",
    "# Verify insertion\n",
    "cur.execute(f\"SELECT COUNT(*), dataset_split FROM {table_name} GROUP BY dataset_split\")\n",
    "results = cur.fetchall()\n",
    "print(\"\\nDatabase contents:\")\n",
    "for count, split in results:\n",
    "    print(f\"  {split}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Tag Prediction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from BERTopic topics to assigned tags\n",
    "# For each BERTopic topic, find the most common assigned tag in training set\n",
    "topic_to_tag_mapping = {}\n",
    "\n",
    "for topic in train_df['bertopic_topic'].unique():\n",
    "    if topic == -1:  # Skip outlier topic\n",
    "        continue\n",
    "    \n",
    "    # Get all documents in this topic\n",
    "    topic_docs = train_df[train_df['bertopic_topic'] == topic]\n",
    "    \n",
    "    # Find most common tag\n",
    "    most_common_tag = topic_docs['primary_tag'].mode()[0] if len(topic_docs) > 0 else \"unknown\"\n",
    "    topic_to_tag_mapping[topic] = most_common_tag\n",
    "\n",
    "print(f\"Created topic-to-tag mapping for {len(topic_to_tag_mapping)} topics\")\n",
    "print(\"\\nSample mappings:\")\n",
    "for topic, tag in list(topic_to_tag_mapping.items())[:10]:\n",
    "    print(f\"  Topic {topic} -> Tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict tags for test set based on BERTopic topics\n",
    "test_df['predicted_tag'] = test_df['bertopic_topic'].map(topic_to_tag_mapping)\n",
    "\n",
    "# Handle outliers and unknown topics\n",
    "test_df['predicted_tag'] = test_df['predicted_tag'].fillna('unknown')\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = test_df['primary_tag'].tolist()\n",
    "y_pred = test_df['predicted_tag'].tolist()\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# Calculate per-tag accuracy\n",
    "print(\"\\nPer-Tag Accuracy:\")\n",
    "for tag in sorted(test_df['primary_tag'].unique()):\n",
    "    tag_mask = test_df['primary_tag'] == tag\n",
    "    tag_correct = (test_df[tag_mask]['primary_tag'] == test_df[tag_mask]['predicted_tag']).sum()\n",
    "    tag_total = tag_mask.sum()\n",
    "    tag_accuracy = tag_correct / tag_total if tag_total > 0 else 0\n",
    "    print(f\"  {tag}: {tag_accuracy:.4f} ({tag_correct}/{tag_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = sorted(list(set(y_true + y_pred)))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix: Assigned Tags vs Predicted Tags')\n",
    "plt.ylabel('True Tag')\n",
    "plt.xlabel('Predicted Tag')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Train set\n",
    "axes[0].hist(train_df['bertopic_prob'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('BERTopic Probability Distribution - Train Set')\n",
    "axes[0].set_xlabel('Topic Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(train_df['bertopic_prob'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {train_df[\"bertopic_prob\"].mean():.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Test set\n",
    "axes[1].hist(test_df['bertopic_prob'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('BERTopic Probability Distribution - Test Set')\n",
    "axes[1].set_xlabel('Topic Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(test_df['bertopic_prob'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean: {test_df[\"bertopic_prob\"].mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_probability_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Topic probability distribution saved as 'topic_probability_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Log Metrics to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow recorder\n",
    "print(\"Initializing MLflow recorder...\")\n",
    "mlflow_recorder = MLflowRecorder(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# Log parameters\n",
    "params = {\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"min_topic_size\": MIN_TOPIC_SIZE,\n",
    "    \"n_neighbors\": N_NEIGHBORS,\n",
    "    \"n_components\": N_COMPONENTS,\n",
    "    \"pages_extracted\": PAGES_TO_EXTRACT,\n",
    "    \"train_test_split\": f\"{int((1-TRAIN_TEST_SPLIT)*100)}/{int(TRAIN_TEST_SPLIT*100)}\",\n",
    "    \"total_documents\": len(df_files),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"num_topics\": len(set(topics)) - 1,\n",
    "    \"embedding_dim\": train_embeddings.shape[1]\n",
    "}\n",
    "\n",
    "mlflow_recorder.log_params(params)\n",
    "\n",
    "# Log evaluation metrics\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1,\n",
    "    \"avg_topic_prob_train\": float(train_df['bertopic_prob'].mean()),\n",
    "    \"avg_topic_prob_test\": float(test_df['bertopic_prob'].mean()),\n",
    "    \"num_outliers_train\": int((train_df['bertopic_topic'] == -1).sum()),\n",
    "    \"num_outliers_test\": int((test_df['bertopic_topic'] == -1).sum())\n",
    "}\n",
    "\n",
    "mlflow_recorder.log_metrics(metrics)\n",
    "\n",
    "# Log artifacts (visualizations)\n",
    "print(\"\\nLogging artifacts...\")\n",
    "mlflow_recorder.log_artifact('confusion_matrix.png')\n",
    "mlflow_recorder.log_artifact('topic_probability_distribution.png')\n",
    "\n",
    "# Set tags\n",
    "mlflow_recorder.set_tag(\"model_type\", \"bertopic\")\n",
    "mlflow_recorder.set_tag(\"task\", \"tag_prediction\")\n",
    "mlflow_recorder.set_tag(\"bucket\", BUCKET_NAME)\n",
    "\n",
    "# End run\n",
    "mlflow_recorder.end_run()\n",
    "\n",
    "print(\"\\nMLflow logging complete!\")\n",
    "print(f\"View results at: {os.getenv('MLFLOW_TRACKING_URI')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "print(\"=\" * 60)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "misclassified = test_df[test_df['primary_tag'] != test_df['predicted_tag']].copy()\n",
    "print(f\"\\nTotal misclassifications: {len(misclassified)} out of {len(test_df)} \"\n",
    "      f\"({len(misclassified)/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nMost common misclassifications:\")\n",
    "    misclass_pairs = misclassified.groupby(['primary_tag', 'predicted_tag']).size().sort_values(ascending=False)\n",
    "    print(misclass_pairs.head(10))\n",
    "    \n",
    "    # Analyze low-confidence predictions\n",
    "    print(\"\\nLow confidence predictions (prob < 0.5):\")\n",
    "    low_conf = test_df[test_df['bertopic_prob'] < 0.5]\n",
    "    print(f\"Count: {len(low_conf)} ({len(low_conf)/len(test_df)*100:.2f}%)\")\n",
    "    \n",
    "    if len(low_conf) > 0:\n",
    "        low_conf_accuracy = (low_conf['primary_tag'] == low_conf['predicted_tag']).mean()\n",
    "        print(f\"Accuracy for low-confidence predictions: {low_conf_accuracy:.4f}\")\n",
    "    \n",
    "    # Analyze high-confidence predictions\n",
    "    print(\"\\nHigh confidence predictions (prob >= 0.5):\")\n",
    "    high_conf = test_df[test_df['bertopic_prob'] >= 0.5]\n",
    "    print(f\"Count: {len(high_conf)} ({len(high_conf)/len(test_df)*100:.2f}%)\")\n",
    "    \n",
    "    if len(high_conf) > 0:\n",
    "        high_conf_accuracy = (high_conf['primary_tag'] == high_conf['predicted_tag']).mean()\n",
    "        print(f\"Accuracy for high-confidence predictions: {high_conf_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExperiment Name: {EXPERIMENT_NAME}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Embedding Dimension: {train_embeddings.shape[1]}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total documents: {len(df_files)}\")\n",
    "print(f\"  Train: {len(train_df)} ({len(train_df)/len(df_files)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_df)} ({len(test_df)/len(df_files)*100:.1f}%)\")\n",
    "print(f\"\\nBERTopic:\")\n",
    "print(f\"  Topics discovered: {len(set(topics)) - 1}\")\n",
    "print(f\"  Avg topic probability (train): {train_df['bertopic_prob'].mean():.4f}\")\n",
    "print(f\"  Avg topic probability (test): {test_df['bertopic_prob'].mean():.4f}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"1. Try different embedding models (multilingual vs monolingual)\")\n",
    "print(\"2. Experiment with different page extraction ranges (1-5 pages)\")\n",
    "print(\"3. Adjust BERTopic parameters (min_topic_size, n_gram_range)\")\n",
    "print(\"4. Consider fine-tuning the embedding model on your domain\")\n",
    "print(\"5. Implement ensemble methods combining multiple models\")\n",
    "print(\"6. Add text preprocessing (cleaning, normalization)\")\n",
    "print(\"\\nAll metrics and artifacts logged to MLflow!\")\n",
    "print(f\"View at: {os.getenv('MLFLOW_TRACKING_URI')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
